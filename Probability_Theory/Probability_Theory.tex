% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames*,x11names*}{xcolor}
%
\documentclass[]{article}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}\pagecolor[RGB]{28,30,38} \color[RGB]{213,216,218}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={Probability Theory},
  pdfauthor={Kexing Ying},
  colorlinks=true,
  linkcolor=Maroon,
  filecolor=Maroon,
  citecolor=Blue,
  urlcolor=red,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage[margin = 1.5in]{geometry}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
\usepackage{tikz}
\usepackage{physics}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{esint}
\usepackage[ruled,vlined]{algorithm2e}
\theoremstyle{definition}
\newtheorem{theorem}{Theorem}
\newtheorem{definition*}{Definition}
\newtheorem{prop}{Proposition}
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem*{remark}{Remark}
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{proposition}{Proposition}[section]
\newtheorem{example}{Example}[section]
\newcommand{\diag}{\mathop{\mathrm{diag}}}
\newcommand{\Arg}{\mathop{\mathrm{Arg}}}
\newcommand{\hess}{\mathop{\mathrm{Hess}}}
% the redefinition for the missing \setminus must be delayed
\AtBeginDocument{\renewcommand{\setminus}{\mathbin{\backslash}}}

\title{Probability Theory}
\author{Kexing Ying}

\begin{document}
\maketitle

{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{2}
\tableofcontents
}
\newpage

\section{Review of Measure Theory}

Modern probability theory is based on measure theory and we will in this section 
recall some notions from measure theory.

\begin{definition}[Algebra]
  Given a set \(\Omega\), a set of subsets \(\mathcal{A}\) of \(\Omega\) is an 
  algebra if \(\Omega \in \mathcal{A}\) and \(\mathcal{A}\) is closed under 
  finite union and complements.

  It follows straight away that an algebra is also closed under finite intersections.
\end{definition}

\begin{definition}[Finitely Additive Measure]
  A function \(\mu : \mathcal{A} \to [0, \infty]\) where \(\mathcal{A}\) is an algebra, 
  is a finitely additive measure if for any disjoint sets \(A, B \in \mathcal{A}\),
  \[\mu(A \cup B) = \mu(A) + \mu(B).\]
\end{definition}

\begin{definition}[\(\sigma\)-Algebra]
  A \(\sigma\)-algebra \(\mathcal{F}\) is an algebra that is closed under countable 
  unions.

  Similarly, it follows that \(\mathcal{F}\) is closed under countable intersections.
\end{definition}

\begin{definition}[Measure]
  A function \(\mu : \mathcal{F} \to [0, \infty]\) where \(\mathcal{F}\) is a 
  \(\sigma\)-algebra, is a \(\sigma\)-additive measure (or simply measure)
  if given a sequence of pairwise disjoint sets \(A_1, A_2, \dots\) of \(\mathcal{F}\), 
  we have 
  \[\mu\left(\bigcup_{i = 1}^\infty A_i\right) = \sum_{i = 1}^\infty \mu(A_n).\]
  We call a measure a probability measure if \(\mu(\Omega) = 1\).
\end{definition}

\begin{definition}[\(\sigma\)-Finite Measure]
  A measure \(\mu\) is said to be \(\sigma\)-finite if there exists a sequence of 
  pairwise disjoint sets \(A_1, A_2, \dots\) of \(\mathcal{F}\), such that 
  \(\bigcup_{i = 1}^\infty A_i = \Omega\) and for all \(i\), \(\mu(A_i) < \infty\).
\end{definition}

\begin{definition}[Probability Space]
  A probability space is the triple \((\Omega, \mathcal{F}, \mathbb{P})\) consisting 
  of a set \(\Omega\), a \(\sigma\)-algebra \(\mathcal{F}\) on \(\Omega\) and 
  \(\mathbb{P}\) a probability measure on \(\mathcal{F}\).

  We call elements of \(\mathcal{F}\) (i.e. a \(\mathcal{F}\)-measurable set) an event.
\end{definition}

\begin{proposition}[Continuity of Measures]
  Let \((A_n)_{n \in \mathbb{N}} \subseteq \mathcal{F}\), then 
  \begin{itemize}
    \item (continuity from below) if \((A_n)\) is increasing, then 
      \[\mathbb{P}\left(\bigcup_{n = 1}^\infty A_n\right) = \lim_{n \to \infty} \mathbb{P}(A_n).\]
    \item (continuity from above) if \((A_n)\) is decreasing, then 
      \[\mathbb{P}\left(\bigcap_{n = 1}^\infty A_n\right) = \lim_{n \to \infty} \mathbb{P}(A_n).\]
  \end{itemize}
  We recall the the finiteness of the measure is vital for continuity from below 
  while continuity from above is also valid for general measures.
\end{proposition}
\begin{proof}
  Exercise.
\end{proof}

\begin{proposition}
  A finitely additive probability measure on the \(\sigma\)-algebra \(\mathcal{F}\) 
  is a probability measure if and only if it is continuous at 0.
\end{proposition}
\begin{proof}
  The forward direction follows from above so we will prove the reverse. 
  Suppose \(\mu\) is finitely additive and for any decreasing \((A_n) \subseteq \mathcal{F}\)
  with \(\bigcap A_n = \varnothing\), we have \(\lim_{n \to \infty} \mu(A_n) = 0\).
  Then, \(\mu\) is continuous from below, and so for any sequence of disjoint 
  sets \((B_n)\), we have \((C_n) := (\bigcup_{i = 1}^n B_i)\) is a sequence of increasing 
  sets and thus, 
  \[\mathbb{P}\left(\bigcup_{i = 1}^\infty B_i \right) 
    = \mathbb{P}\left(\bigcup_{i = 1}^\infty C_i \right) 
    = \lim_{n \to \infty} \mathbb{P}(C_n)
    = \lim_{n \to \infty} \mathbb{P}\left(\bigcup_{i = 1}^n B_i \right)
    = \lim_{n \to \infty} \sum_{i = 1}^n \mathbb{P}(B_i)\]
  implying \(\mu\) is \(\sigma\)-additive and so, \(\mu\) is a measure.
\end{proof}

\begin{proposition}
  Given a collection \(\{\mathcal{F}_i\}_{i \in I}\) \(\sigma\)-algebras of \(\Omega\),  
  \(\bigcap_{i \in I} \mathcal{F}_i\) is also a \(\sigma\)-algebra on \(\Omega\).
\end{proposition}

\begin{definition}[\(\sigma\)-Algebra Generated By Sets]
  Given a collection of subsets \(S\) of \(\Omega\), the \(\sigma\)-algebra generated 
  by \(S\) is 
  \[\sigma(S) := \bigcap \{\mathcal{F} \text{ a }\sigma\text{-algebra} \mid S \subseteq \mathcal{F}\}.\]
\end{definition}

\begin{definition}[Borel \(\sigma\)-Algebra]
  Given a topological space \((X, \mathcal{T})\), the Borel \(\sigma\)-algebra 
  on \(X\) is \(\mathcal{B}(X) := \sigma(\mathcal{T})\).
\end{definition}

\begin{definition}[Product \(\sigma\)-Algebra]
  Given measurable spaces \((\Omega_1, \mathcal{F}_1), (\Omega_2, \mathcal{F}_2)\), 
  the product \(\sigma\)-algebra on \(\Omega_1 \times \Omega_2\) is 
  \[\mathcal{F}_1 \otimes \mathcal{F}_2 := 
    \sigma(\mathcal{F}_1 \times \mathcal{F}_2) = 
    \sigma(\{A_1 \times A_2 \mid A_1 \in \mathcal{F}_1, A_2 \in \mathcal{F}_2\}).\]
\end{definition}

\begin{definition}[Cylindrical \(\sigma\)-Algebra]
  A set \(C \subseteq \mathbb{R}^\infty\) is said to be cylindrical if is of the 
  form 
  \[C = \{x \in \mathbb{R}^\infty \mid (x_1, \cdots, x_n) \in C_n\}\]
  where \(C_n \in \mathcal{B}(\mathbb{R}^n)\). The set of cylindrical sets 
  \(\mathcal{B}(\mathbb{R}^\infty)\) form 
  a \(\sigma\)-algebra on \(\mathbb{R}^\infty\) and is called the cylindrical 
  \(\sigma\)-algebra.
\end{definition}

\begin{definition}[Consistent]
  The sequence of measures \(\mathbb{P}_n\) on \((\mathbb{R}^n, \mathcal{B}(\mathbb{R}^n))\)
  is said to be consistent if for all \(n \in \mathbb{N}\), 
  \(\mathbb{P}_{n + 1}(B_n \times \mathbb{R}) = \mathbb{P}_n(B_n)\) 
  for all \(B_n \in \mathcal{B}(\mathbb{R}^n)\).
\end{definition}

\begin{theorem}[Kolmogorov]
  Given any consistent sequence of measures \(\mathbb{P}_n\), there exists a unique 
  probability measure \(\mathbb{P}\) on \((\mathbb{R}^\infty, \mathcal{B}(\mathbb{R}^\infty))\) 
  such that, 
  \[\mathbb{P}(\{x \in \mathbb{R}^\infty \mid (x_1, \cdots, x_n) \in \mathcal{B}_n\}) = 
    \mathbb{P}_n(B_n)\]
  for all \(n \ge 1\), \(B_n \in \mathcal{B}(\mathbb{R}^n)\).
\end{theorem}
\begin{proof}
  Simply define the inner measure on the generating sets as claimed and use 
  the Caratheodory extension (which provides both existence and uniqueness).
\end{proof}

Recall that a nondecreasing function \(g\) on \(\mathbb{R}\) is continuous 
up to possibly countably many discontinuities of the first kind. Furthermore, 
the derivative \(g'\) exists \(\lambda\)-a.e. (where \(\lambda\) is the Lebesgue 
measure on \(\mathbb{R}\).

\begin{proposition}
  Let \((\mathbb{R}, \mathcal{B}(\mathbb{R}), \mathbb{P})\) be a probability space. 
  Defining \(F(x) := \mathbb{P}(-\infty, x]\), we have 
  \begin{itemize}
    \item \(F\) is nondecreasing;
    \item \(\lim_{x \to -\infty} F(x) = 0\) and \(\lim_{x \to \infty}F(x) = 1\);
    \item \(F\) is continuous on the right.
  \end{itemize}
\end{proposition}
\begin{proof}
  Clear by the monotonicity, continuity of measures (from above).
\end{proof}

\begin{definition}[Distribution Function]
  Any function \(F : \mathbb{R} \to [0, 1]\) satisfying the above three properties 
  is said to be a distribution function on \(\mathbb{R}\).
\end{definition}

It is clear that any probability measure induces a distribution. On the other hand 
the converse is also true.

\begin{proposition}
  Given a distribution function \(F\), there exists a unique probability measure 
  \(\mathbb{P}\) on \((\mathbb{R}, \mathcal{B}(\mathbb{R}))\)
  such that \(F(x) = \mathbb{P}(-\infty, x]\) for all \(x \in \mathbb{R}\).
\end{proposition}
\begin{proof}
  Use Caratheodory extension theorem on the algebra \(\{(-\infty, x] \mid x \in \mathbb{R}\}\) 
  mapping \((-\infty, x] \mapsto F(x)\). The uniqueness of the probability measure 
  follows by the uniqueness of the Caratheodory extension.
\end{proof}

\begin{definition}[Null-set]
  Given a measure \(\mu\), a set \(S \subseteq \Omega\) is a null-set if there 
  exists some measurable set \(N \subseteq \Omega\) with measure 0 such that \(S \subseteq N\).
\end{definition}

\begin{definition}[Complete Measure]
  A measure \(\mu\) is complete if every \(\mu\)-null set is measurable.
\end{definition}

If a measure on the \(\sigma\)-algebra \(\Sigma\) is not complete, we may complete 
the \(\sigma\)-algebra by extending \(\Sigma\) to 
\[\overline{\Sigma} := \sigma(\Sigma \cup \{N \mid N \text{ is a null-set}\}).\]
Clearly, the null-sets will have measure 0. 

We note that the probability space \((\mathbb{R}, \mathcal{B}(\mathbb{R}), \mathbb{P})\) 
is not complete as there exists subsets of a Borel null-set which are not Borel. 
With this in mind, we denote the completion of \(\mathcal{B}(\mathbb{R})\) by 
\(\mathcal{M}(\mathbb{R})\) and we say \(\mathbb{P}\) is the Lebesgue-Stieltjes 
measure.

Recall, that in elementary probability theory, we considered three types of 
distributions, namely discrete, absolutely continuous and singular continuous.
Let us now consider them again in a formal measure theoretic setting. 

\begin{itemize}
  \item (Discrete) A random variable \(X : \Omega \to \mathbb{R}\) is said to 
    have discrete distribution if there exists some countable (including finite) 
    set \(A \subseteq \mathbb{R}\), such that for all \(E \in \mathcal{B}(\mathbb{R})\), 
    the push-forward measure satisfies 
    \[X_*\mathbb{P}(E) = \sum_{x \in A} p(x) \delta_x(E)\]
    where \(p(x) = X_*\mathbb{P}(\{x\})\) and \(\delta_x\) is the 
    Dirac measure at \(x\).

    We note that a distribution function \(F\) corresponds to a discrete random variable 
    if and only if for all \(x_0 \in \mathbb{R}\), 
    \[F(x_0) = \sum_{x \in A \cap \{\le x_0\}} p(x).\]
    It is clear that \(\sum_A p(x) = 1\) since \(\sum_A p(x) = X_*\mathbb{P}(\mathbb{R}) = 1\).
  \item (Absolutely continuous) A random variable \(X : \Omega \to \mathbb{R}\) is 
    said to be absolutely continuous if \(X_* \mathbb{P} = f \lambda\) for 
    some Lebesgue integrable function \(f\) and \(\lambda\) denotes Lebesgue measure.
    Thus, the distribution function corresponding to \(X\) satisfies 
    \[F(x) = \int_{(-\infty, x]} f \dd \lambda.\]
    In particular, recalling the Radon-Nikodym theorem, we have \(X\) is absolutely 
    continuous if and only if \(X_*\mathbb{P} \ll \lambda\) (hence the name ``absolutely continuous'').
\end{itemize}

Before introducing the last type of distribution, let us consider the following 
definition.

\begin{definition}[Concentrated]
  A measure \(\mu\) on the measurable space \(X\) is said to be concentrated on a 
  measurable set \(A\) if \(\mu(E) = 0\) for all \(E \subseteq X \setminus A\).
\end{definition}

\begin{itemize}
  \item (Singular continuous) A random variable \(X : \Omega \to \mathbb{R}\) is 
  singular continuous if its distribution function \(F\) is continuous and 
  \(X_* \mathbb{P}\) is concentrated on a set \(A\) of Lebesgue measure 0 for 
  which \(F'(x) = 0\) for all \(x \in A\) almost everywhere. 

  We note that, since \(\lambda(A) = 0\), \(X_* \mathbb{P} \perp \lambda\) by 
  the set \(A\) (hence the name ``singular''). Moreover, by continuity, 
  \(X_* \mathbb{P}(\{x\}) = 0\) for all \(x \in \mathbb{R}\) in contrast to the 
  discrete measure.
\end{itemize}

Analogous to the Lebesgue decomposition of measures, we may decompose any distribution 
function in to a discrete, absolutely continuous and singular continuous distribution.

\begin{theorem}[Hahn Decomposition for Distributions]
  Given a distribution function \(F\), there exists \(a_1 + a_2 + a_3 = 1\) 
  and \(F_{\text{disc}}, F_{\text{ac}}, F_{\text{sc}}\) discrete, absolutely 
  continuous and singular continuous distribution functions respectively, such that 
  \[F = a_1 F_{\text{disc}} + a_2 F_{\text{ac}} + a_3 F_{\text{sc}}.\]
\end{theorem}
\begin{proof}
  Recalling the refinement of the Lebesgue decomposition where we may decompose 
  a measure \(\mu\) with 
  \[\mu = \mu_d + \mu_a + \mu_s\]
  where \(\mu_d\) is a discrete measure, \(\mu_a \ll \lambda\) and \(\mu_s\) is 
  singular continuous (i.e. mutually singular with respect to the Lebesgue measure 
  and \(\mu_s\{x\}\) for all \(x\)). Thus, by simply taking the the decomposition 
  of the measure corresponding to \(F\) (i.e. \(X_*\mathbb{P}\)), we obtain the 
  required decomposition after normalization.
\end{proof}

\newpage
\section{Random Variables}

We will continue to let \((\Omega, \mathcal{F}, \mathbb{P})\) be a probability space. 

\begin{definition}[Random Variable]
  A function \(\xi : \Omega \to \mathbb{R}\) is said to be a random variable if 
  it is \(\mathcal{F}\)-measurable (i.e. for any \(B \in \mathcal{B}(\mathbb{R})\), 
  we have \(\xi^{-1}(B) \in \mathcal{F}\)).  
\end{definition}

While we have already introduced the notion of a distribution within the previous 
section, we will present it here again for organization.

\begin{definition}[Distribution of a Random Variable]
  Given a random variable \(\xi\), the distribution of \(\xi\) is the push-forward 
  measure of \(\mathbb{P}\) along \(\xi\). Furthermore, the distribution function 
  corresponding to \(\xi\) is 
  \[F(x) := X_* \mathbb{P}(- \infty, x].\]
\end{definition}

\begin{definition}
  Given a random variable \(\xi\), we define \(\mathcal{F}_\xi \subseteq \mathcal{F}\) 
  to be the \(\sigma\)-algebra 
  \[\mathcal{F}_\xi := \{\xi^{-1}(B) \mid B \in \mathcal{B}(\mathbb{R})\}.\]
  This is the least \(\sigma\)-algebra for which \(\xi\) is measurable.
\end{definition}

We will recall some standard results about measurable functions. All proofs are 
left as exercises and can be found in the second year measure theory notes.

\begin{lemma}
  If \(\mathcal{B}(\mathbb{R}) = \sigma(\mathcal{D})\) for a collection of sets 
  \(\mathcal{D}\), \(\xi\) is a random variable if \(\xi^{-1}(D) \in \mathcal{F}\) 
  for all \(D \in \mathcal{D}\).
\end{lemma}

\begin{lemma}
  Given random variables \(f, g\) and \(c \in \mathbb{R}\), 
  \(f + g, f - g, c \cdot f, |f|, fg\), \(\max(f, g)\), and \(\min(f, g)\) are all random variables.
  Furthermore, if \(g(x) \neq 0\) for all \(x\), then \(f / g\) is also a random variable. 
\end{lemma}

\begin{lemma}
  If \((f_n)\) is a sequence of random variables, then 
  \[\sup_n f_n, \inf_n f_n, \lim_n f_n\]
  are random variables if they exist.
\end{lemma}

\begin{lemma}
  If \(\xi\) is a random variable and \(f : \mathbb{R} \to \mathbb{R}\) is continuous, 
  then \(f(\xi)\) is a random variable.
\end{lemma}

\begin{definition}[Simple Function]
  A random variable \(\xi\) is simple if there exists a partition of \(\Omega\), 
  \(D_1, \cdots, D_n\) such that 
  \[\xi(\omega) = \sum_{i = 1}^n x_i 1_{D_i}(\omega)\]
  for some \(x_1, \cdots, x_n\) for all \(\omega \in \Omega\).
\end{definition}

\begin{lemma}
  For any non-negative random variable \(\xi\), there exists a sequence of 
  nondecreasing simple random variables \((\xi_n)\) such that for all \(\omega \in \Omega\),
  \[\xi_n(\omega) \uparrow \xi(\omega).\]
\end{lemma}

\begin{definition}[Random Vector]
  A function \(\xi : \Omega \to \mathbb{R}^n\) is a random vector if it is 
  measurable. Again, we define its distribution to be its push-forward measure.
\end{definition}

\begin{lemma}
  \(\xi : \Omega \to \mathbb{R}^n\) is a random vector if and only if 
  \(\xi_i := \text{pr}_i \circ \xi\) is a random variable for all \(i = 1, \cdots, n\)
  (where \(\text{pr}_i : \mathbb{R}^n \to \mathbb{R}\) is the \(i\)-th projection function).
\end{lemma}

\begin{definition}[Independent Random Variables]
  Two random variables \(\xi, \eta : \Omega \to \mathbb{R}\) are said to be 
  independent if 
  \[(\xi, \eta)_* \mathbb{P} = \xi_* \mathbb{P} \otimes \eta_* \mathbb{P}.\]
  Since, to check that two measures are equal, it suffices to check equality on 
  generating sets, \(\xi, \eta\) are independent if for all \(A, B \in \mathcal{B}(\mathbb{R})\),
  \[\mathbb{P}(\xi \in A, \eta \in B) = \mathbb{P}(\xi \in A) \mathbb{P}(\eta \in B).\]
\end{definition}

Let us quickly recall the construction of the Lebesgue integral. 
\begin{enumerate}
  \item Define the Lebesgue integral for simple functions.
  \item Define the Lebesgue integral for non-negative functions by taking the limit 
    of the Lebesgue integral of the monotone sequence of simple functions which converge 
    to the said function.
  \item Define the Lebesgue integral for general real-valued functions \(f\) by 
    taking \(\int f = \int f^+ - \int f^-\) if \(\int |f| < \infty\).
\end{enumerate}

\begin{definition}[Expectation]
  Given a random variable \(\xi : \Omega \to \mathbb{R}\), the expectation of \(\xi\) 
  is simply
  \[\mathbb{E}(\xi) := \int \xi \dd \mathbb{P}\]
  if it exists. Furthermore, we say \(\xi\) is integrable if \(\mathbb{E}(|\xi|) < \infty\). 
\end{definition}

\begin{proposition}
  Let \(\xi, \eta\) be integrable random variables and let \(c \in \mathbb{R}\), 
  then 
  \begin{itemize}
    \item \(\mathbb{E}(c) = c\);
    \item \(\mathbb{E}(\xi + \eta) = \mathbb{E}(\xi) + \mathbb{E}(\eta)\);
    \item \(\xi \le \eta\) a.e. implies \(\mathbb{E}(\xi) \le \mathbb{E}(\eta)\); 
    \item \(\xi = \eta\) a.e. implies \(\mathbb{E}(\xi) = \mathbb{E}(\eta)\); 
    \item \(\xi \ge 0\) a.e. and \(\mathbb{E}(\xi) = 0\) implies \(\xi = 0\) a.e.
  \end{itemize}
\end{proposition}
\begin{proof}
  Follows directly from the properties of the Lebesgue integral.
\end{proof}

Let us recall some convergence theorems for the Lebesgue integral.

\begin{theorem}[Dominated Convergence Theorem]
  Let \((\xi_n)\) be a sequence of random variables such that \(\xi_n \to \xi\) 
  almost everywhere. If there exists some integrable \(\eta\) such that 
  \(|\xi_n| \le \eta\) for all \(n\), then, \(\xi\) is integrable and 
  \[\lim_{n \to \infty} \mathbb{E}(\xi_n) = \mathbb{E}(\xi).\]
\end{theorem}

\begin{theorem}[Monotone Convergence Theorem]
  Let \((\xi_n)\) be a sequence of non-negative increasing random variables. Then, 
  \[\lim_{n \to \infty} \mathbb{E}(\xi_n) = \mathbb{E} \lim_{n \to \infty} \xi_n.\]
  We note that the right hand side limit always exists since for all \(\omega \in \Omega\),
  \(\xi_n(\omega)\) is increasing any bounded by \(\infty\).
\end{theorem}

We remark that the monotone convergence theorem applies if there exists some 
random variable \(\eta\) such that \(\mathbb{E}(\eta) > -\infty\) such that 
\(\eta \le \xi_n\) for all \(n\) by considering \(\xi_n - \eta\).

\begin{corollary}
  If \((\eta_n)\) is a sequence of non-negative random variables, then 
  \[\sum_{i = 1}^\infty \mathbb{E}(\eta_i) = \mathbb{E}\left(\sum_{i = 1}^\infty \eta_i\right).\]
\end{corollary}

\begin{corollary}[Fatou's lemma]
  Let \(\xi_n\) be a sequence of non-negative random variables. Then, 
  \[\mathbb{E}(\liminf_{n} \xi_n) \le \liminf_n \mathbb{E} \xi_n.\]
\end{corollary}
\begin{proof}
  Apply the monotone convergence theorem to \(\lambda_n := \inf_{k > n} \xi_k\).
\end{proof}

Again, the non-negative condition can be replaced by the existence of some random 
variable \(\eta\) such that \(\mathbb{E}(\eta) > -\infty\) and \(\eta \le \xi_n\)
for all \(n\). On the other hand, if \(\mathbb{E}(\eta) < \infty\) and \(\xi_n \le \eta\),
the theorem holds with limit supremum instead. 

We note that in all above theorems, the statement still holds by replacing \(\Omega\) 
with any measurable set by restricting the measure onto that set.

\begin{theorem}[Change of Variables]
  Given a random variable \(\xi\), a measurable function \(g : \mathbb{R} \to \mathbb{R}\) 
  and a measurable set \(A\), we have 
  \[\int_A g \dd \xi_* \mathbb{P} = \int_{\xi^{-1}(A)} g \circ \xi \dd \mathbb{P},\]
  where both integrals either exist or not exist simultaneously.
\end{theorem}
\begin{proof}
  Apply usual method where one first prove the statement for indicator functions. 
  Then, it follows that it holds for simple functions by the linearity of the 
  integral. Finally, for any non-negative measurable function, we take a sequence 
  of monotonically increasing simple functions, and apply the monotone convergence 
  theorem. For arbitrary functions, the result follows by taking \(f = f^+ - f^-\).
\end{proof}

\begin{corollary}[Law of the Unconscious Statistician]
  Given a random variable \(\xi\) and a measurable function \(g : \mathbb{R} \to \mathbb{R}\),
  \[\mathbb{E}(g(\xi)) = \int_{\mathbb{R}} g \dd \xi_* \mathbb{P}.\]
\end{corollary}

\begin{corollary}
  Let \(g : \mathbb{R} \to \mathbb{R}\) be measurable, then, if 
  \(\xi\) be a discrete random variable, 
  \[\mathbb{E}(g(\xi)) = \sum_{x \in A}g(x) p(x).\]
  On the other hand, if \(\xi\) is absolutely continuous, i.e. there exists 
  some \(f\) such that \(f\lambda = \xi_* \mathbb{P}\), then 
  \[\mathbb{E}(g(\xi)) = \int_{\mathbb{R}}g(x)f(x) \lambda(\dd x).\]
\end{corollary}
\begin{proof}
  In the discrete case, we have 
  \[\mathbb{E}(g(\xi)) = \int g \dd \left(\sum_{x \in A}p(x) \delta_x\right) = 
    \sum_{x \in A} p(x) \int g \dd \delta_x.\]
  By considering \(\int g \dd \delta_x = \int_{\{x\}} g \dd \delta_x + 
  \int_{\mathbb{R} \setminus \{x\}} g \dd \delta_x = \delta_x(\{x\})g(x) + 0 = g(x)\).
  We have 
  \[\mathbb{E}(g(\xi)) = \sum_{x \in A}g(x) p(x),\]
  as required.

  On the other hand, if \(f\lambda = \xi_* \mathbb{P}\), we have
  \[\mathbb{E}(g(\xi)) = \int_{\mathbb{R}} g \dd (f \lambda) = \int_{\mathbb{R}} g(x)f(x) \lambda(\dd x)\]
  as required.
\end{proof}

\begin{theorem}[Fubini's Theorem]
  Let \((E_1, \Sigma_1, \mu_1), (E_2, \Sigma_2, \mu_2)\) be \(\sigma\)-finite 
  measure spaces. Then, for any \(\Sigma_1 \otimes \Sigma_2\)-measurable 
  functions \(g : E_1 \times E_2 \to \mathbb{R}\), \(g(\cdot, y_0)\) is 
  \(\Sigma_1\)-measurable for all \(y_0 \in E_2\), and \(g(x_0, \cdot)\) is 
  \(\Sigma_2\)-measurable for all \(x_0 \in E_1\). Furthermore, 
  \(\int_{E_1} g \dd \mu_1, \int_{E_2} g \dd \mu_2\) are \(\Sigma_2\) and \(\Sigma_1\)-measurable 
  respectively. Finally, if \(\int |g| \dd \mu_1 \otimes \mu_2 < \infty\),
  then, 
  \[\int g \dd \mu_1 \otimes \mu_2 = \int \left(\int g(x, y) \mu_2(\dd y)\right) \mu_1(\dd x) 
   = \int \left(\int g(x, y) \mu_1(\dd x)\right) \mu_2(\dd y).\]
\end{theorem}

\subsection{Inequalities}

\begin{lemma}[Jensen's Inequality]
  Let \(\xi\) be an integrable random variable and let \(g : \mathbb{R} \to \mathbb{R}\)
  be a measurable, convex function, then,
  \[g(\mathbb{E} \xi) \le \mathbb{E}g(\xi).\]
\end{lemma}
\begin{proof}
  Recall that the function \(g\) is convex if for all \(x_0 \in \mathbb{R}\), there 
  exists some \(\lambda\) such that \(g(x) \ge g(x_0) + (x - x_0) \lambda\)
  (graphically, \(\lambda\) is the slope (more accurately, a subderivative) of 
  \(g\) at \(x_0\) and so, the inequality is saying that the graph lies above the 
  tangent line). 

  Setting \(x = \xi\) and \(x_0 = \mathbb{E}\xi\). Then, the above inequality becomes 
  \[g(\xi) \ge g(\mathbb{E}\xi) - (\xi - \mathbb{E}\xi)\lambda.\]
  Thus, applying the expectation to both sides results in the required inequality 
  by the linearity of the integral.
\end{proof}

\begin{corollary}[Lyapunov's Inequality]
  Let \(\xi\) be a random variable and let \(0 < s < t\) be real numbers, then 
  \[\mathbb{E}(|\xi|^s)^{1 / s} \le \mathbb{E}(|\xi|^t)^{1 / t}.\]
\end{corollary}
\begin{proof}
  Use Jensen's inequality with \(g(x) = |x|^{t / s}\). 

  Alternatively, setting \(\eta = |\xi|^s\), by HÃ¶lder's inequality, we have 
  \[\|\xi^s\|_1 =  \|\eta\|_1 \le \|\eta\|_{t / s} = \|\xi\|^s_t.\]
  Thus, taking both sides to the power of \(1 / s\), we obtain 
  \(\|\xi\|_s = (\|\xi^s\|_1)^{1 / s} \le (\|\xi\|^s_t)^{1 / s} = \|\xi\|_t\) as required.
\end{proof}

\begin{proposition}[Markov Inequality]
  Let \(\xi \ge 0\) be an integrable random variable and let \(c > 0\). Then 
  \[\mathbb{P}(\xi \ge c) \le \frac{\mathbb{E}(\xi)}{c}.\]
\end{proposition}
\begin{proof}
  \(\mathbb{E}(\xi) \ge \mathbb{E}(\xi\mathbf{1}_{\xi \ge c}) \ge 
    c \mathbb{E}(\mathbf{1}_{\xi \ge c}) = c\mathbb{P}(\xi \ge c).\)
\end{proof}

\begin{definition}[Variance]
  The variance (or dispersion) of a random variable \(\xi\) is defined to be 
  \[V_\xi := \mathbb{E}[(\xi - \mathbb{E}\xi)^2]\]
  and we define \(\sigma := \sqrt{V_\xi}\) the standard deviation of \(V_\xi\).
\end{definition}

By expanding the definition, we fine \(V_\xi = \mathbb{E}\xi^2 - (\mathbb{E}\xi)^2\).

\begin{definition}[Covariance]
  The covariance of random variables \(\xi\) and \(\eta\) is defined to be 
  \[\text{cov}(\xi, \eta) := \mathbb{E}[(\xi - \mathbb{E})(\eta - \mathbb{E}\eta)].\]
\end{definition}

\begin{proposition}
  For random variables \(\xi, \eta\), we have 
  \begin{itemize}
    \item \(V_{\xi + \eta} = V_\xi + V_\eta + 2 \text{cov}(\xi, \eta)\);
    \item if \(\text{cov}(\xi, \eta) = 0\), then \(V_{\xi + \eta} = V_\xi + V_\eta\).
  \end{itemize}
\end{proposition}
\begin{proof}
  Clear.
\end{proof}

\begin{proposition}[Chebyshev's Inequality]
  Let \(\xi\) be a integrable random variable. Then, for all \(\epsilon > 0\), 
  \[\mathbb{P}(|\xi - \mathbb{E}\xi| \ge \epsilon) \le \frac{V_\xi}{\epsilon^2}.\]
\end{proposition}
\begin{proof}
  By Markov inequality, for \(\xi \ge 0\), we have 
  \[\mathbb{P}(\xi \ge \epsilon) = \mathbb{P}(\xi^2 \ge \epsilon^2) \le \frac{\mathbb{E}\xi^2}{\epsilon^2}.\]
  Thus, by replacing \(\xi\) by \(|\xi - \mathbb{E}\xi|\), we have the required inequality. 
\end{proof}

\begin{proposition}[Exponential Chebyshev's Inequality]
  Let \(\xi \ge 0\) be a random variable and let \(\epsilon, t > 0\) such that \(\xi, e^{t\xi}\) are
  integrable. Then, 
  \[\mathbb{P}(\xi \ge \epsilon) \le e^{-t\epsilon} \mathbb{E}(e^{t\xi}).\]
\end{proposition}
\begin{proof}
  We observe, by Markov inequality 
  \[\mathbb{P}(\xi \ge \epsilon) = \mathbb{P}(e^{t\xi} \ge e^{t\epsilon}) \le \frac{E(e^{t\xi})}{e^{t\epsilon}}.\]
\end{proof}

\begin{proposition}[Tail Probability]
  Let \(\xi \ge 0\) be an integrable rndom variable. Then, 
  \[\mathbb{E}(\xi) = \int_{[0, \infty)} \mathbb{P}(\xi \ge x) \lambda(\dd x).\]
\end{proposition}
\begin{proof}
  By change of variable, we have,
  \[\mathbb{E}\xi = \int_\Omega \xi \dd \mathbb{P} = \int_{[0, \infty)} x (\xi_*\mathbb{P})(\dd x) = 
    \int_{[0, \infty)}\int_{[0, x]} \lambda(\dd t) (\xi_* \mathbb{P})(\dd x).\]
  Then, by Fubini's theorem to the function \(g : (t, x) \mapsto \mathbf{1}_{[0, x]}(t)\), we have 
  \[\int_{[0, \infty)}\int_{[0, x]} \lambda(\dd t) (\xi_* \mathbb{P})(\dd x) = 
    \int_{[0, \infty)^2} g(t, x) \lambda(\dd t) (\xi_* \mathbb{P})(\dd x) = 
    \int_{[0, \infty)} \mathbb{P}(\xi \ge x) \lambda(\dd x)\]
  as required.
\end{proof}

\begin{definition}[Normal Random Variable]
  A random variable \(\xi\) is said to be norm if \(\xi_* \mathbb{P} = f\lambda\) 
  where 
  \[f(x) = \frac{1}{\sqrt{2\pi \sigma^2}} e^{-\frac{(x - m)^2}{2\sigma^2}}\]
  for some \(m \in \mathbb{R}, \sigma > 0\). We denote this by \(\xi \sim \mathcal{N}(m, \sigma^2)\).
\end{definition}

\begin{proposition}
  Let \(\xi \sim \mathcal{N}(m, \sigma^2)\). Then, \(\mathbb{E}\xi = m\) and \(V_\xi = \sigma^2\),
  and so, a normal random variable is fully determined by its mean and variance.
\end{proposition}
\begin{proof}
  Exercise.
\end{proof}

\begin{definition}[Moment]
  Given a random variable \(\xi\), we define the \(k\)-th moment of \(\xi\) to be 
  \(\mathbb{E}(\xi^{k})\).
\end{definition}

\subsection{Transformation of Random Variables}

Let \(F_\xi(x)\) be a distribution function of a random variable \(\xi\). Then,
if \(\phi\) is a real valued continuous function, we would like to consider the 
distribution of \(\eta\) where \(\eta = \phi(\xi)\). An easy observation is 
that
\[F_\eta(y) = \mathbb{P}(\eta \le y) = \mathbb{P}(\eta \in \phi^{-1}(-\infty, y]) 
  = \int_{\phi^{-1}(-\infty, y]} \dd(\xi_*\mathbb{P}).\]
As one might expect, elementary methods from first year probability are sufficient 
for most cases we encounter (consider the case \(\phi\) is linear of quadratic).

Suppose now that \(\xi\) is absolutely continuous (and so, has a density by 
Radon-Nikodym), we would like to find the density of \(\eta := \phi(\xi)\).
Assume first that \(\xi(\Omega) \in I\) where \(I\) is a finite or infinite open 
interval and let \(\phi\) be continuously differentiable and strictly increasing 
on \(I\). Denoting \(h(y) = \phi^{-1}(\{y\})\) which is well-defined and differentiable, 
for all \(y \in \phi(I\), 
\[F_\eta(y) = \mathbb{P}(\eta \le y) = \mathbb{P}(\xi \le \phi^{-1}(\{y\})) 
  = \int_{(-\infty, h(y)]} f_\xi \dd \lambda = \int_{(-\infty, y]} f_\xi(h(z)) h'(z) \lambda(\dd z)\]
where \(f_\xi\) is the density of \(\xi\). Hence, the density of \(\eta\) is 
\(f_\xi(h(y)) h'(y) = f_\xi(h(y)) |h'(y)|\). Similarly, if \(\phi\) is strictly 
decreasing, \(\eta\) remain to have the density \(f_\xi(h(y)) |h'(y)|\). 
With this in mind, we may obtain the density for a large class of 
transformations by de compositing the density into a strictly increasing and 
decreasing parts.

In the case that \((\xi, \eta)\) is a random vector with joint distribution 
\(F\) and let \(\phi\) be a continuous function. Then, \(\phi(\xi, \eta)\) 
has distribution
\[F_{\phi(\xi, \eta)}(z) = \int_{\phi^{-1}(-\infty, z]} \dd ((\xi, \eta)_* \mathbb{P}).\]

\subsection{Independence}

We recall that two random variables \(\xi, \eta\) are said to be independent if 
\((\xi, \eta)_* \mathbb{P} = \xi_* \mathbb{P} \otimes \eta_* \mathbb{P}\). 
Thus, if \(F_\xi, F_\eta\) are distributions of \(\xi\) and \(\eta\) respectively, 
then, \(F_{(\xi, \eta)}(x, y) = F_\xi(x) F_\eta(y)\) for all \(x, y \in \mathbb{R}\) 
where \(F_{(\xi, \eta)}\) is a distribution of the random vector \((\xi, \eta)\).

\begin{proposition}
  If \(\xi, \eta\) are independent random variables, then the distribution of 
  \(\xi + \eta\) is 
  \[F_{\xi + \eta}(z) = 
    \int_{\mathbb{R}} F_\eta(z - x) \xi_*\mathbb{P}(\dd x) =
    \int_{\mathbb{R}} F_\xi(z - y) \eta_*\mathbb{P}(\dd y).\]
\end{proposition}
\begin{proof}
  By taking \(\phi : \mathbb{R}^2 \to \mathbb{R} : (x, y) \mapsto x + y\), we have 
  \[F_{\xi + \eta}(z) = \int_{\phi^{-1}(-\infty, z]} \dd ((\xi, \eta)_* \mathbb{P})
   = \int_{\phi^{-1}(-\infty, z]} \dd (\xi_* \mathbb{P} \otimes \eta_* \mathbb{P})\]
  by independence. By considering that \((x, y) \in \phi^{-1}(-\infty, z]\) if and only 
  if \(x + y \le < z\), we have by Fubini's theorem,
  \[\begin{split}
    \int_{\phi^{-1}(-\infty, z]} \dd (\xi_* \mathbb{P} \otimes \eta_* \mathbb{P}) & = 
    \int_{\mathbb{R}^2} \mathbf{1}_{x + y \le z} \dd (\xi_* \mathbb{P} \otimes \eta_* \mathbb{P})\\
    & = \int_{\mathbb{R}} \left(\int_{\mathbb{R}} \mathbf{1}_{x + y \le z} \eta_*\mathbb{P}(\dd y)\right) \xi_*\mathbb{P}(\dd x)\\
    & = \int_{\mathbb{R}} F_\eta(z - x) \xi_*\mathbb{P}(\dd x) 
      = \int_{\mathbb{R}} F_\xi(z - y) \eta_*\mathbb{P}(\dd y). 
  \end{split}\]
\end{proof}

Recalling the definition of the convolution of a function, we may reformulate the 
above as the following corollaries.

\begin{definition}[Convolution]
  Given two real-valued functions \(f, g : \Omega \to \mathbb{R}\), we define the
  convolution of \(f\) with \(g\) by 
  \[f * g := t \mapsto \int_{\mathbb{R}} f(x) g(t - x) \mu(\dd x).\]
\end{definition}

\begin{corollary}
  The distribution function of the sum of two independent random variables is 
  the convolution of their distribution functions.
\end{corollary}

\begin{corollary}
  If \(\xi, \eta\) are independent absolutely continuous random variables, then,
  the density of \(\xi + \eta\) is the convolution of their densities.
\end{corollary}

\begin{proposition}
  Let \(\xi, \eta\) be independent integrable random variables. Then, \(\xi \cdot \eta\) 
  is integrable and \(\mathbb{E}(\xi \cdot \eta) = \mathbb{E}(\xi) \mathbb{E}(\eta)\).
\end{proposition}
\begin{proof}
  It is clearly true for indicator functions and so, we may extend to simple function 
  by the linearity of expectation. Hence, by monotone convergence, the statement is 
  true for non-negative random variables and hence true for arbitrary random variables 
  by taking \(\xi = \xi^+ - \xi^-\) and \(\eta = \eta^+ - \eta^-\).  
\end{proof}

\begin{definition}
  Random variables \(\xi, \eta\) are said to be uncorrelated if \(\text{cov}(\xi, \eta) = 0\).
\end{definition}

\begin{proposition}
  Independent random variables are uncorrelated.
\end{proposition}
\begin{proof}
  Clear since \(\text{cov}(\xi, \eta) = \mathbb{E}(\xi \cdot \eta) - \mathbb{E}\xi \mathbb{E}\eta\). 
\end{proof}

We note that the converse is not true. Namely, uncorrelated does not imply independence.

\subsection{Weak Law of Large Numbers}

Consider \((\Omega_n, \mathcal{A}, \mathbb{P}_n)\) as 
a (finite) probability space such that 
\[\Omega_n := \{\omega \mid \omega = (a_1, \cdots, a_n), a_i \in \{0, 1\}\},
  \mathcal{A} = \mathcal{P}(\Omega_n)\]
and 
\[\mathbb{P}_n(\{\omega\}) = p(\omega) = p^{\sum_{i = 1}^n a_i}q^{n - \sum_{i = 1}^n a_i}\]
for some \(0 < p < 1\) and \(q = 1 - p\). 
Let \(\xi_1, \cdots, \xi_n : \Omega_n \to \{0, 1\}\) be random variables such that 
\(\xi_i(\omega) = a_i\). It is easy to check that \(\xi_i\) are independent and 
identically distributed (iid.). Indeed, 
\[(\xi_i)_*\mathbb{P}_n(\{1\}) = \mathbb{P}_n(\{\omega \mid a_j(\omega) = 1\}) = 
  p\sum_{k = 0}^{n - 1} {{n - 1} \choose k} p^kq^{n - k} = p\]
and \((\xi_i)_*\mathbb{P}_n(\{0\}) = (\xi_i)_*\mathbb{P}_n(\{1\}^c) = 1 - p = q\). 

Now defining \(S_n := \sum_{i = 1}^n \xi_i\), we observe that 
\[\mathbb{E}S_n = \sum_{i = 1}^n \mathbb{E}\xi_i = \sum_{i = 1}^n p = np.\]
Thus, the expectation of \(\frac{1}{n} S_n\) is simply \(p\). We now ask what is 
\(|\frac{1}{n}S_n(\omega) - p|\). Immediately, we observe that \(|\frac{1}{n}S_n(\omega) - p|\) 
cannot tends to 0 point-wise since \(S_n(0) = 0\) for all \(n\). Nonetheless, 
we observe that \(\mathbb{P}_n(S_n = 0) = q^n \to 0\) as \(n \to \infty\). 

Recalling that the Kolmogorov extension theorem, as \(\{\mathbb{P}_n\}\) is consistent, 
there exists a unique probability measure \(\mathbb{P}\) on \((\mathbb{R}^\infty, \mathcal{B}(\mathbb{R}^\infty))\) 
such that \(\mathbb{P}(\xi \in \mathbb{R}^\infty \mid (\xi_1, \cdots, \xi_n) \in B_n) = 
\mathbb{P}_n((\xi_1, \cdots, \xi_n) \in B_n)\). With this measure in mind, using 
Chebyshev's inequality, and the fact that they are independent and hence, uncorrelated,
we obtain 
\[\mathbb{P}\left(\left|\frac{1}{n}S_n - p\right| \ge \epsilon\right) \le \frac{V(\frac{1}{n}S_n)}{\epsilon^2}
  = \frac{1}{\epsilon^2} \sum_{j = 1}^n V\left(\frac{1}{n} \xi_i\right) = \frac{1}{n^2 \epsilon^2}\sum_{i = 1}^n V_{\xi_i}
  = \frac{npq}{n^2 \epsilon^2} \to 0\]
as \(n \to \infty\). Thus, \(\frac{1}{n}S_n\) converges to \(p\) in measure (probability).

This fact is known as Bernoulli's law of large numbers. By noting that we only used 
the uncorrelated fact, we obtain a more general theorem.

In general (with arbitrary \(\xi_i\)), we call the quantity \(\frac{1}{n} S_n\) the 
time average and the (weak) law of large numbers tells us the time average converges 
in measure to the space average \(\mathbb{E}\xi_i\). 

\begin{theorem}[Weak Law of Large Numbers]
  let \(\xi_1, \xi_2, \cdots\) be integrable random variables. Defining 
  \(S_n^{(c)} = \sum_{i = 1}^n (\xi_i - \mathbb{E}\xi_i)\) (we note that 
  \(\mathbb{E}S_n^{(c)} = 0\)). Then, if \(\xi_1, \xi_2 \cdots\) are uncorrelated 
  and for all \(i\), \(V_{\xi_i} \le C\) for some \(C > 0\), for all \(\epsilon > 0\),
  \[\lim_{n \to \infty} \mathbb{P}\left(\left|\frac{1}{n} S_n^{(c)}\right| \ge \epsilon\right) = 0\]
  i.e. \(\frac{1}{n}S^{(c)}_n \to 0\) in measure.
\end{theorem}
\begin{proof}
  By Chebyshev's inequality, as \(\xi_i\) are uncorrelated,
  \[\mathbb{P}\left(\left|\frac{1}{n}S_n^{(c)}\right| \ge \epsilon\right) \le 
  \frac{V(\frac{S_n^{(c)}}{n})}{\epsilon^2} \le \frac{c}{n\epsilon^2} \to 0\]
  as \(n \to \infty\).
\end{proof}

\begin{corollary}
  If \(\xi_1, \xi_2, \cdots\) integrable iid. random variables such that \(V_{\xi_i} < \infty\), 
  then, \(\frac{1}{n}\sum_{i = 1}^n \xi_i\) converges to \(\mathbb{E}\xi_i\) in measure.
\end{corollary}

We would also like to consider the limiting behaviour of the distribution. 
Let us first recall the big and small-O notation. 

Given sequences of functions \((f_n), (g_n)\), we denote \(g_n = O_{n \to \infty}(|f_n|)\) 
if \(|g_n / f_n|\) is eventually bounded, i.e. there exists some \(c, N\) such that 
for all \(n \ge N\), \(|g_n| \le c|f_n|\). On the other hand, we denote 
\(g_n = o_{n \to \infty}(|f_n|)\) if \(\lim_{n \to \infty} |g_n / f_n| = 0\). 

Returning to our example of the Bernoulli random variables, we have the following 
result.

\begin{theorem}[Local Limit Theorem]
  For any \(0 < p < 1\), 
  \[\max_{0 \le k \le n} \left|\mathbb{P}(S_n = k) - \frac{1}{\sqrt{2\pi np(1 - p)}}e^{- \frac{x^2}{2p(1 - p)}} \right| 
  = o\left(\frac{1}{\sqrt{n}}\right),\]
  where \(x = x_{k, n} := \frac{k - np}{\sqrt{n}}\).
\end{theorem}
\begin{proof}
  Let \(A_n > 0\) such that \(A_n = o(n)\) (e.g. \(A_n = n^\epsilon\) for some \(0 < \epsilon < 1\)).
  Then, let \(k \in \mathbb{N}\) such that \(|x_{k, n}| \le A_n / \sqrt{n}\) and so, 
  \[np - A_n \le k \le np + A_n.\]
  Then, \(k, n- k \to \infty\) as \(n \to \infty\) and using Stirling's formula,
  \[\begin{split}
    \mathbb{P}(S_n = k) & = \frac{n!}{k!(n - k)!}p^k(1 - p)^{n - k}\\ 
    & = \frac{\sqrt{2\pi n}}{\sqrt{2\pi k}{\sqrt{2\pi (n - k)}}}
        e^{n \log n - k \log k - (n - k)\log(n - k)}p^k (1 - p)^{n - k}\left(1 + O\left(\frac{1}{n}\right)\right).
  \end{split}\]
  Then, by noting that \(np + x \sqrt{n} = k = np(1 + O(A_n / n))\) and 
  \(n(1 - p) - x\sqrt{n} = n - k = n(1 - p)(1 + O(A_n / n))\), we have 
  \[\begin{split}
    \mathbb{P}(S_n = k) & =
    \sqrt{\frac{n}{np2\pi n(1 - p)}}\left(1 + O\left(\frac{A_n}{n}\right)\right)
    e^{n \log n - (np + x\sqrt{n})(\log(np) + \frac{x}{p\sqrt{n}} - \frac{x^2}{2p^2\sqrt{n}} + O\left(\frac{x}{\sqrt{n}}\right)^3)}\\
    & e^{-(n(1 - p) - x\sqrt{n}(\log(n(1 - p) - \frac{x}{(1 - p)\sqrt{n}} - \frac{x^2}{2(1 - p)^2 n} + O\left(\frac{x}{\sqrt{n}}\right)^3)))}
    e^{k\log p + (n - k)\log(1 - p)}\\
    & = \sqrt{\frac{1}{2\pi n p(1 - p)}} \left(1 + O\left(\frac{A_n}{n}\right)\right)
    e^{-(np + x\sqrt{n})(\frac{x}{p\sqrt{n}} - \frac{x^2}{2p^2 n} + O\left(\frac{x}{\sqrt{n}}\right)^3)}\\
    & e^{(n(1 - p) - x\sqrt{n})(\frac{x}{(1 - p)\sqrt{n}} - \frac{x^2}{2(1 - p)^2 n} + O\left(\frac{x}{\sqrt{n}}\right)^3)}\\
    & = \sqrt{\frac{1}{2\pi n p(1 - p)}} \left(1 + O\left(\frac{A_n}{n}\right)\right)\\
    & e^{-\sqrt{n} x + \frac{x^2}{2p} - \frac{x^2}{p} +\frac{x^3}{2p^2 \sqrt{n}}}
    e^{O\left(\frac{x}{\sqrt{n}}\right)^3 + \sqrt{n}x + \frac{x^2}{2(1 - p)} - \frac{x^2}{1 - p} -\frac{x^3}{2(1 - p^2)\sqrt{n}}}\\
    & = \frac{1 + O\left(\frac{A_n}{n}\right)}{\sqrt{2\pi n p(1 - p)}} 
      e^{-\frac{x^2}{2}(\frac{1}{p} + \frac{1}{1 - p}) + O\left(\frac{A_n^3}{\sqrt{n}^3 \sqrt{n}}\right)}\\
    & = \frac{1}{\sqrt{2\pi p(1 - p)n}} e^{-\frac{x^2}{2p(1 - p)}}\left(1 + O\left(\frac{A_n^3}{n^2}\right) + 
      O\left(\frac{A_n}{n}\right)\right).
  \end{split}\]
  Hence, choosing \(A_n = n^{7 / 12}\), the result follows for \(np - A_n \le k \le np + A_n\). 
  Finally, by monotonicity of \(P(S_n = k)\), we have the result follows for all \(k \le n\).
\end{proof}

\begin{theorem}[de Moivre-Laplace Central Limit Theorem]
  For any \(0 < p < 1\) and \(x \in \mathbb{R}\), 
  \[\lim_{n \to \infty} \mathbb{P}\left(\frac{S_n - np}{\sqrt{np(1 - p)}} \le x\right) = \Phi(x),\]
  where \(\Phi(x)\) is the distribution of the normal random variable with parameters 
  \(m = 0, \sigma^2 = 1\).
\end{theorem}
\begin{proof}
  Using the local limit theorem, we have
  \[\begin{split}
    \mathbb{P}\left(\frac{S_n - np}{\sqrt{np(1 - p)}} \le x\right) & = 
    \mathbb{P}(S_n \le np + x\sqrt{np(1 - p)})
     = \sum_{k = 0}^{\lfloor np + x\sqrt{np(1 - p)}\rfloor} \mathbb{P}(S_n = k)\\
      & = o(1) + \sum_{k = \lfloor np - A_n \rfloor}^{\lfloor np + x\sqrt{np(1 - p)}\rfloor}
      \frac{1}{\sqrt{2\pi}\sqrt{np(1 - p)}}e^{-\frac{1}{2}\left(\frac{k - np}{\sqrt{np(1 - p)}}\right)^2}
  \end{split}\]
  which converges to \(\int_{(-\infty, x]} \frac{1}{\sqrt{2\pi}} e^{-y^2 / 2} \lambda(\dd y)\) as 
  required.
\end{proof}

Finally, as a remark, again using Sterling's formula, we see that 
\[\begin{split}
  \mathbb{P}(S_n = k) & = \frac{n(n - 1) \cdots (n - k + 1)}{k!}\left(\frac{\lambda}{n} + o\left(\frac{1}{n}\right)\right)^k\\
  & = \left(1 - \frac{\lambda}{n} + o\left(\frac{1}{n}\right)\right)^{n - k} \to \frac{1}{k!} \lambda^ke^{-\lambda}
\end{split}\]
as \(n \to \infty\) where the limit is the Poisson distribution. 

\newpage
\section{Convergence and Characteristic Functions}

\subsection{Different Modes of Convergence}

Let us recall some different notions of convergence in probability theory.

\begin{definition}
  A sequence of random variables \((\xi_n)_n\) is said to converge 
  \begin{itemize}
    \item almost surely if \(\xi_n(\omega) \to \xi(\omega)\) almost everywhere 
    on \(\Omega\);
    \item in probability if for all \(\epsilon > 0\),
    \[\lim_{n \to \infty} \mathbb{P}(|\xi_n - \xi| \ge \epsilon) = 0;\]
    \item in \(L^p\) for \(1 \le p < \infty\) (also known as convergence in mean 
    of order \(p\)) if 
    \[\lim_{n \to \infty} \mathbb{E}(|\xi_n - \xi|^p) = 0;\]
    \item in distribution (or weakly) if for any bounded continuous function \(f\),
    \[\mathbb{E}(f(\xi_n)) \to \mathbb{E}(f(\xi)).\]
    Namely, this condition requires laws \((\xi_n)_* \mathbb{P}\) converges weakly 
    to \(\xi_*\mathbb{P}\). Note that this does not require \((\xi_n)\) to be 
    defined on the same probability space.
  \end{itemize}
\end{definition}

\begin{proposition}
  We have the following relations between the different notions of convergence.
  \begin{itemize}
    \item Convergence almost surely implies convergence in probability.
    \item Convergence in probability implies the existence of a convergent almost 
      surely subsequence.
    \item Convergence in \(L^p\) implies convergence in probability.
  \end{itemize}
\end{proposition}
\begin{proof}
  Exercise.
\end{proof}

\begin{theorem}
  Let \(\xi_n \ge 0\) be a sequence of integrable random variable which converges 
  almost surely to \(\xi\). Then, if \(\mathbb{E}\xi_n \to \mathbb{E}\xi\), 
  we have \(\xi_n \to \xi\) in \(L^1\).
\end{theorem}
\begin{proof}
  We see 
  \[\mathbb{E}|\xi_n - \xi| = \mathbb{E}(\xi - \xi_n)\mathbf{1}_{\xi \ge \xi_n} 
    + \mathbb{E}(\xi_n - \xi)\mathbf{1}_{\xi_n > \xi} = 
    2\mathbb{E}(\xi - \xi_n) \mathbf{1}_{\xi \ge \xi_n} + \mathbb{E}(\xi_n - \xi).\]
  Thus, the result follows by considering \(0 \le (\xi - \xi_n)\mathbf{1}_{\xi \ge \xi_n} \le \xi\), 
  and applying dominated convergence. 
\end{proof}

\begin{proposition}
  Convergence in probability implies convergence in distribution for integrable 
  random variables.
\end{proposition}
\begin{proof}
  Let \((\xi_n)\) be a sequence of integrable random variables which converges 
  in probability to \(\xi\). Fix \(\epsilon > 0\) and let \(f\) be a continuous 
  bounded function such that \(|f(x)| \le C\) for all \(x\). By Markov's 
  inequality, there exists some \(N\) such that 
  \[\mathbb{P}(|\xi| > N) \le \frac{\epsilon}{4c}.\]
  Now, as \(f\) is continuous, there exists some \(\delta > 0\) such that 
  \(|f(x) - f(y)| < \epsilon / 2\) for all \(|x|\le N\), \(|x - y| \le \delta\).
  Then, 
  \[\begin{split}
    \mathbb{E}(|f(\xi_n) - f(\xi)|) & = 
    \mathbb{E}(|f(\xi) - f(\xi_n)|)\mathbf{1}_{|\xi_n - \xi| \le \delta, |\xi| \le n}\\
    & + \mathbb{E}(|f(\xi) - f(\xi_n)|)\mathbf{1}_{|\xi_n - \xi| \le \delta, |\xi| > n}
    + \mathbb{E}(|f(\xi) - f(\xi_n)|)\mathbf{1}_{|\xi_n - \xi| > \delta}\\
    & \le \frac{\epsilon}{2} + 2c \frac{\epsilon}{4c} + 2c \mathbb{P}(|\xi_n - \xi| > \delta)\\
    & \le \frac{\epsilon}{2} + \frac{\epsilon}{2} + 2c \mathbb{P}(|\xi_n - \xi| > \delta)
  \end{split}\]
  where the last equality follows as \(\xi_n\) converges to \(\xi\) in probability.
  Thus, as the \(\epsilon\) is arbitrary, 
  \[\lim_{n \to \infty} \mathbb{E}(|f(\xi_n) - f(\xi)|) = 0.\]
\end{proof}

\subsection{Weak Convergence of Measures}

\begin{theorem}[Portmanteau Theorem]
  Let \((\xi_n)\) be a sequence of integrable random variables. Then, the 
  following are equivalent:
  \begin{enumerate}
    \item \(\xi_n\) converges in distribution to \(\xi\).
    \item \(\limsup_n \mathbb{P}(\xi_n \in B) \le \mathbb{P}(\xi \in B)\) for all closed \(B\).
    \item \(\liminf_n \mathbb{P}(\xi_n \in A) \ge \mathbb{P}(\xi \in A)\) for all open \(A\).
    \item \(\lim_n \mathbb{P}(\xi_n \in C) = \mathbb{P}(\xi \in C)\) for all borel \(C\) with 
      \(\mathbb{P}(\xi \in \partial C) = 0\).
    \item The distribution functions \(F_{\xi_n}\) converges to \(F_\xi\) at all 
      points of continuity of \(F_\xi\).
  \end{enumerate}
\end{theorem}
\begin{proof}
  \((1 \implies 2)\). Let \(B\) be closed and let \(f := \mathbf{1}_B\) and let 
  \(f_\epsilon(x) := g(\epsilon^{-1} \text{dist}(x, B))\) where 
  \[g(t) := \begin{cases}
    1, \ & t\le 0,\\
    1 - t, \ & 0 \le t \le 1,\\
    0, \ & t > 1.
  \end{cases}\]
  We observe that \(f_\epsilon(x)\) is 0 if \(x\) is not in a \(\epsilon\)-neighbourhood
  of \(B\). In particular, defining \(B_\epsilon := \{x \mid \text{dist}(x, B)\}\), 
  \(f_\epsilon(B_\epsilon^c) = \{0\}\). We note that \(B_\epsilon \downarrow B\) 
  as \(\epsilon \downarrow 0\) and so \(f_\epsilon \to f\). Now, by considering
  \[\begin{split}
    \mathbb{P}(\xi_n \in B) & = \int f \dd (\xi_n)_*\mathbb{P}
      \le \int f_\epsilon \dd (\xi_n)_*\mathbb{P},
  \end{split}\]
  we have 
  \[\begin{split}
    \limsup_n \mathbb{P}(\xi_n \in B) 
    & \le \limsup_n \int f_\epsilon \dd (\xi_n)_*\mathbb{P} 
      = \int f_\epsilon \dd \xi_*\mathbb{P} \le \mathbb{P}(\xi \in B_\epsilon)
  \end{split}\]
  where the equality follows by convergence in distribution and the second 
  inequality follows as \(f_\epsilon(B_\epsilon^c) \le 1\). Thus, by the continuity 
  of measure, \(\mathbb{P}(\xi \in B_\epsilon) \to \mathbb{P}(\xi \in B)\) as 
  \(\epsilon \downarrow 0\) and hence, 
  \(\limsup_n \mathbb{P}(\xi_n \in B) \le \mathbb{P}(\xi \in B)\) as required.

  \((2 \iff 3)\). Follows taking the complement. 

  \((2, 3 \implies 4)\). Taking any borel set \(C\), we recall that 
  \(\overline{C} = C \cup \partial C\) and \(C^\circ = C \setminus \partial C\).
  Then, as \(\mathbb{P}(\xi \in \partial C) = 0\), we have 
  \[\limsup_n \mathbb{P}(\xi_n \in C) \le \limsup_n \mathbb{P}(\xi_n \in \overline{C}) 
    \le \mathbb{P}(\xi \in \overline{C}) = \mathbb{P}(\xi \in C),\]
  and 
  \[\liminf_n \mathbb{P}(\xi_n \in C) \ge \liminf_n \mathbb{P}(\xi_n \in C^\circ) 
    \ge \mathbb{P}(\xi \in C^\circ) = \mathbb{P}(\xi \in C).\]
  Hence, \(\lim_n \mathbb{P}(\xi_n \in C) = \mathbb{P}(\xi \in C)\) as required.

  \((4 \implies 5)\). Obvious from 5.

  \((5 \implies 1)\). This implication is a bit more complicated. The general idea 
  uses the fact that if 5 holds, then there exists a probability space 
  \((\Omega', \mathcal{F}', \mathbb{P}')\) and a sequence of random variables 
  \((\eta_n)\) on this probability space such that 
  \((\xi_n)_*\mathbb{P}_n = (\eta_n)_*\mathbb{P}'\) (I'm writing 
  \(\mathbb{P}_n\) to especially indicate \(\xi_n\) are on possibly different 
  measurable spaces) \(\xi_*\mathbb{P} = \eta_* \mathbb{P}'\) and \(\eta_n \to \eta\) 
  almost surely.

  Assuming this theorem (which will will prove below), 
  let \(f\) be a continuous bounded function. As 
  \(f\) is continuous, \(f(\eta_n) \to f(\eta)\) almost surely. Now, as 
  they are bounded, we may apply dominated convergence such that 
  \[\mathbb{E}(f(\xi_n)) = \mathbb{E}(f(\eta_n)) \to \mathbb{E}(f(\eta)) = \mathbb{E}(f(\xi))\]
  where the equalities follows by change of variables.
\end{proof}

\begin{lemma}
  Let \(\xi\) be a random variable with distribution function \(F_\xi\) and let 
  \(U\) be a uniformly distributed random variable on the interval \([0, 1]\) 
  where we denote 
  \[F^{-1}(u) := \sup \{y \mid F_\xi(y) < u\}.\]
  Then, \(\xi_* \mathbb{P} = (F^{-1}(U))_* \lambda\).
\end{lemma}
\begin{proof}
  This follows as \(u \le F_\xi(x) \iff F_\xi^{-1}(u) \le x\) since \(F_\xi\) 
  is nondecreasing and continuous from the right. Indeed, 
  if \(u \le F_\xi(x)\) and suppose for contradiction that \(F_\xi^{-1}(u) > x\),
  then, by definition, \(\sup \{y \mid F_\xi(y) < u\} > x\) and so, 
  \(\sup \{y \mid F_\xi(y) < u \le F_\xi(x)\} > x\). However, this is not possible 
  as \(F_\xi\) is nondecreasing, thus a contradiction.
  On the other hand, if \(x \le F_\xi^{-1}(u) = \sup \{y \mid F_\xi(y) < u\}\), 
  as \(F_\xi\) is nondecreasing and continuous from the right, 
  \(F_\xi(x) \ge F_\xi(\sup \{y \mid F_\xi(y) < u\}) \ge u\).
\end{proof}

\begin{theorem}[Single Probability Space Theorem]
  Let \((\xi_n)\) be a sequence of integrable random variables such that the 
  distribution functions \(F_{\xi_n}\) converges to \(F_\xi\) at all 
  points of continuity of \(F_\xi\). Then, there exists a probability space 
  \((\Omega', \mathcal{F}', \mathbb{P}')\) and a sequence of random variables 
  \((\eta_n)\) on this probability space such that 
  \((\xi_n)_*\mathbb{P}_n = (\eta_n)_*\mathbb{P}'\), 
  \(\xi_*\mathbb{P} = \eta_* \mathbb{P}'\) and \(\eta_n \to \eta\) 
  almost surely.
\end{theorem}
\begin{proof}
  Let \((\Omega', \mathcal{F}', \mathbb{P}') := ([0, 1], \mathcal{B}([0, 1]), \lambda)\).
  Then, it suffices to prove that \(F_{\xi_n}^{-1}(u) \to F_\xi^{-1}(u)\) for all 
  \(u\) such that \(|F_\xi^{-1}(u)| < \infty\) and the measure of all other points 
  is zero as they form a countable set (nondecreasing function can have at most 
  countably many discontinuities). Indeed, defining \(\eta_n := F_{\xi_n}^{-1}(U)\) 
  converges to \(\eta := F_\xi^{-1}(U)\) almost surely. Then, by the above lemma, 
  the result follows. 

  Suppose \(u\) is a point such that \(|F_\xi^{-1}(u)| < \infty\). Then, for any 
  \(x < F_\xi^{-1}(u)\), \(F(x) < u\) and if \(x\) is a point of continuity 
  of \(F_\xi\), we have \(F_{\xi_n}(x) \to F_\xi(x)\) and so, \(F_{\xi_n}< u\) 
  for sufficiently large \(n\). Thus, \(x \le F_{\xi_n}^{-1}(u)\) implying 
  \(x \le \liminf_n F_{\xi_n}^{-1}(u)\). Now, by taking a sequence \((x_n)\) 
  of points of continuity of \(F_\xi\) such that \(x_n \uparrow F^{-1}(u)\), 
  we obtain \(F^{-1}(u) \le \liminf_n F_{\xi_n}^{-1}(u)\).

  On the other hand, if \(x > F_\xi^{-1}(u)\), we have \(F(x) \ge u\). 
  If \(F_\xi(x) = u\), then we see that \(|F_{\xi}^{-1}(u)| = \infty\). 
  So, \(F_\xi(x) > u\) and from which by the same argument as the less 
  than case, we obtain \(F^{-1}(u) \ge \limsup_n F_n^{-1}(u)\) implying 
  convergence as required.
\end{proof}

\begin{definition}[Relatively Compact]
  A family of probability measures \(\mathcal{P} := 
  \{\mathbb{P}_\alpha \mid \alpha \in A\}\) and the corresponding set of distribution 
  functions \(F_\alpha\) is called relatively compact if every sequence of 
  measures of \(\mathcal{P}\) has a subsequence which converges weakly 
  (not necessary in \(\mathcal{P}\) hence the name ``relatively''). 
\end{definition}

We note that the weak convergence of measures is metrizable with the corresponding 
metric known as the Levy-Prokhorov metric. In this sense, relatively compactness 
of \(\mathcal{P}\) is equivalent to \(\overline{\mathcal{P}}\) is compact in 
this metric space.

Denote \(\mathcal{G}\) the set of functions \(F : \mathbb{R} \to [0, 1]\) which 
is non-decreasing and continuous from the right (and we call functions of 
this form generalised distribution functions), we have the following result.

\begin{theorem}[Helly's Theorem]
  The set \(\mathcal{G}\) is sequentially compact, i.e. any sequence 
  \((F_n) \subseteq \mathcal{G}\) has a subsequence \((F_{n_k})\) such that 
  \(F_{n_k}(x) \to F(x)\) for all \(x \in C_F\).
\end{theorem}
\begin{proof}
  Let us enumerate \(\mathbb{Q}\) with \((q_i)_{i = 1}^\infty\). Then, as 
  \(\{F_n(q_1) \mid n\}\) is bounded, it has a convergent subsequence 
  \(F_{n_k^{(1)}}(q_1) \to f(q_1)\). Similarly, as \(\{F_{n_k^{(1)}}(q_2) \mid n\}\) 
  is bounded, we can find a subsequence \(F_{n_k^{(1)}}\), \(F_{n_k^{(2)}}\) 
  which converges for both \(q_1, q_2\). Then, iterating this process, for all 
  \(i\), we obtain a subsequence of \(F_{n_k^{(i - 1)}}\) which converges 
  at \(q_1, \cdots, q_i\). Thus, by defining \(F_{n_k} := F_{n_k^{(k)}}\), 
  i.e. the diagonal, we have found a subsequence of \(F_n\) which converges 
  at all rational numbers. Let us denote the limit of \(F_{n_k}(q_i)\) as \(f(q_i)\).

  Now, defining 
  \[F(x) := \inf \{f(q) \mid q \in \mathbb{Q}_{> x}\},\]
  I claim that \(F \in \mathcal{G}\), \(F(q) \ge f(q)\) for all \(q \in \mathbb{Q}\)
  and \(F(x) \le f(q)\) for all \(x < q\), \(q \in \mathbb{Q}\). Indeed, 
  the last inequality is clear by construction while the second inequality 
  follows as \(f\) is non-decreasing. Thus, by the definition of \(\inf\), 
  \(F\) is continuous from the right and so, \(F \in \mathcal{G}\).

  Finally, it remains to check that \(F_{n_k}(x) \to F(x)\) at all points of 
  continuity. Let \(x\) be a point of continuity and fix \(\epsilon > 0\). As 
  \(x\) is a point of continuity, there exists some \(y < p < x < q\), such that 
  \[F(x) - \epsilon < F(y) \le F(p) \le F(x) \le F(q) < F(x) + \epsilon.\] 
  Then, as \(F(y) \le f(p)\) for all \(y < p\), \(F(x) - \epsilon < f(p)\). 
  On the other hand, \(F(p) \ge f(p)\), so 
  \[F(x) - \epsilon < f(p) \le F(p) \le F(x) \le f(q) < F(x) + \epsilon.\]
  Thus, for sufficiently large \(k\), 
  \[F(x) - \epsilon < F_{n_k}(p) \le F_{n_k}(x) \le F_{n_k}(q) < F(x) + \epsilon,\]
  implying \(|F_{n_k}(x) - F(x)| < \epsilon\) and hence, 
  \(F_{n_k}(x) \to F(x)\) as required.
\end{proof}

\begin{definition}[Tight]
  A family of probability measures \(\mathcal{P} := 
  \{\mathbb{P}_\alpha \mid \alpha \in A\}\) on \((\mathbb{R}, \mathcal{B}(\mathbb{R}))\) 
  is called tight if for all \(\epsilon > 0\), there exists a compact set 
  \(K \subseteq \mathbb{R}\) such that 
  \[\sup_{\alpha \in A} \mathbb{P}_\alpha(K^c) \le \epsilon.\]
\end{definition}

\begin{theorem}[Prokhorov's Theorem]
  A family of probability measures \(\mathcal{P} := \{\mathbb{P}_\alpha \mid \alpha \in A\}\) 
  is tight if and only if it is relatively compact.
\end{theorem}
\begin{proof}
  Suppose first \(\mathcal{P}\) is relatively compact and suppose for contradiction 
  it is not tight. That is, there exists some \(\epsilon > 0\), for all compact 
  \(K \subseteq \mathbb{R}\), 
  \[\sup_{\alpha \in A} \mathbb{P}_\alpha(K^c) > \epsilon.\]
  In particular, we may choose \(K_n := [-n, n]\) for any \(n\). Thus, 
  for any \(n\), there exists some \(\mathbb{P}_{\alpha_n} \in \mathcal{P}\) 
  such that \(\mathbb{P}_{\alpha_n}(K_n^c) > \epsilon\). Now, since \(\mathcal{P}\)
  is relatively compact, it has a weakly convergent subsequence 
  \(\mathbb{P}_{\alpha_{n_k}} \to \mathbb{P}\) for some probability measure 
  \(\mathbb{P}\). As \(\mathbb{P}_n\) are probability measures, we have 
  for all \(m > k\), 
  \[\mathbb{P}_{\alpha_m}([-k, k]) \le \mathbb{P}_{\alpha_m}([-m, m]) < 1 -\epsilon.\]
  So, by weak convergence, 
  \[\mathbb{P}([-k, k]) = \int \mathbf{1}_{[-k, k]} \dd \mathbb{P} = 
    \lim_{m \to \infty} \int \mathbf{1}_{[-k, k]} \dd \mathbb{P}_{\alpha_{n_m}}
    < 1 - \epsilon.\]
  On the other hand, as \(\mathbb{P}\) is a probability measure,
  \[1 = \mathbb{P}(\mathbb{R}) = 
    \mathbb{P}\left(\bigcup_{k \in \mathbb{N}} [-k, k]\right) = 
    \lim_{k \to \infty} \mathbb{P}([-k, k]) \le 1 - \epsilon < 1,\]
  which is a contradiction. Hence, \(\mathcal{P}\) is tight.

  Suppose now \(\mathcal{P}\) is tight and let \((\mathbb{P}_n) \subseteq \mathcal{P}\) 
  be a sequence of probability measures with corresponding distribution functions 
  \((F_n)\). By Helly's theorem, there exists a subsequence \((F_{n_k})\) which 
  converges to some \(F \in \mathcal{G}\) at all points of continuity of \(F\). 
  It remains to show \(\lim_{x \to - \infty} F(x) = 0\) and 
  \(\lim_{x \to \infty} F(x) = 1\).

  Fix \(\epsilon > 0\). Then, by tightness, there exists some compact \(K\) 
  such that \(\sup \mathbb{P}_{n_k}(K^c) \le \epsilon\). Now, since 
  \(K \subseteq \mathbb{R}\) it is bounded by some \(M\) (choose \(M\) such that 
  \(\pm M\) is a point of continuity of \(F\)) and so, 
  \(K \subseteq [-M, M]\) and 
  \[\sup F_{n_k}(-M) = 
    \sup \mathbb{P}_{n_k}((-\infty, -M]) \le 
    \sup \mathbb{P}_{n_k}([-M, M]^c) \le 
    \sup \mathbb{P}_{n_k}(K^c) \le \epsilon.\]
  Thus, as \(F(M) \le \epsilon\), and as it is non-decreasing, 
  \(\lim_{x \to -\infty} F(x) = 0\). Similarly, we have 
  \[\inf F_{n_k}(M) = \inf \mathbb{P}_{n_k}((-\infty, M]) 
    \ge \inf \mathbb{P}_{n_k}([-M, M]) \ge \inf \mathbb{P}_{n_k}(K) \ge 1 - \epsilon.\]
  Thus, \(\lim_{x \to \infty} F(x) = 1\). Hence, \(\mathbb{P}_{n_k}\) 
  converges to the probability measure corresponding to \(\mathbb{P}\) weakly 
  by Portmanteau's theorem as required.
\end{proof}

\end{document}